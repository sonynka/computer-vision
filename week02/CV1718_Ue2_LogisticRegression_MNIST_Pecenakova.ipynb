{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistische Regression mit MNIST</h1>\n",
    "\n",
    "<p>Die lineare Regression findet eine Linearkombination der Eingangsdaten die die Zieldaten gut approximiert. Mit Hilfe der logistischen Regressionen können auch nicht-lineare Abbildungen gefunden werden. Außerdem eignet sie sich um binäre Aussagen zu treffen. Die heutige Übung verwendet das MNIST Datenset von Yann LeCun, welches Bilder von handgeschriebenen Ziffern beinhaltet. Ein Model der logistische Regression soll mit diesen Daten trainiert und im Anschluss getestet werden. Das fertige System soll in der Lage sein die Ziffer auf einen unbekannten Bild zu benennen.</p>\n",
    "\n",
    "<p>Dieses Jupyter Notebook steht wieder zum <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung2/LogisticRegression_MNIST_Vorlage.ipynb\" target=\"_blank\">download</a> bereit.</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<h2>Vorbereitung</h2>\n",
    "\n",
    "<p>Es ist Sinnvoll zun&auml;chst die Lineare Regression 2D in eine vektorisierte Variante zu &uuml;berf&uuml;hren um diesen Ansatz anhand eines Beispiels zu lernen welches bereits bekannt ist. Die Vektoren/Matrizen-Schreibweise ist f&uuml;r die nachfolgende &Uuml;bung unabdiengbar. Die Idee ist es alle Berechnungen (Prediction, Error usw) ohne for-loops auszurechnen. Das gilt ebenfalls für die partiellen Ableitungen die mit einer Matrix Multiplikation gelöst werden können. Auf diese Weise sind beliebig hoch dimensionale Eingangdaten verarbeitbar.</p>\n",
    "\n",
    "<p><a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung1/#LinearRegression_2D_Vorlage.ipynb\">http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung1/#LinearRegression_2D_Vorlage.ipynb</a></p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<h2>Einleitung</h2>\n",
    "\n",
    "<p>Scikit-learn liefert eine Methode um die MNIST Daten aus dem Internet zu laden. Leider ist deren Server etwas unzuverl&auml;ssig, weshalb wir die Daten selbst von einen HTW Server downloaden.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfileobj\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets.base import get_data_home\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_mnist(data_home=None):\n",
    "    mnist_alternative_url = \"http://home.htw-berlin.de/~hezel/files/data/mnist-original.mat\"    \n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    data_home = os.path.join(data_home, 'mldata')\n",
    "    if not os.path.exists(data_home):\n",
    "        os.makedirs(data_home)\n",
    "    mnist_save_path = os.path.join(data_home, \"mnist-original.mat\")\n",
    "    if not os.path.exists(mnist_save_path):\n",
    "        print(\"Download MNIST to\",mnist_save_path)\n",
    "        mnist_url = urllib.request.urlopen(mnist_alternative_url)\n",
    "        with open(mnist_save_path, \"wb\") as matlab_file:\n",
    "            copyfileobj(mnist_url, matlab_file)\n",
    "    return fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mnist()\n",
    "print(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die erhaltenden Daten teilen wir in Test und Trainingsdaten auf. Wobei wir darauf achten die Daten vorher zu mischen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# input and output data\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target\n",
    "\n",
    "# shuffle data\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Daten Visualisierung</h2>\n",
    "\n",
    "<p>Visualisieren sie von jeder Ziffer 5 Zahlen in einen 10x5 gro&szlig;en Grid. Die Ziffern sind alle in 28x28 Pixel gro&szlig;en graustufen Bildern gespeichert. Jede Zeile von X representieren eine Zahl. Ist der Zeilenindex bekannt, kann die Zeile mit numpy.take(...) extrahiert werden.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# enable interactive plots\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plots 50 images in a 10x5 grid.\n",
    "vis_matrix = np.zeros([10, 5, 28, 28])\n",
    "\n",
    "for number in range(0, 10):\n",
    "    idxs, = np.where(y == number)\n",
    "    idxs= idxs[:5]\n",
    "#     print(idxs)\n",
    "    images = X[idxs].reshape([5, 28, 28])\n",
    "#     print(images.shape)\n",
    "    vis_matrix[number] = images\n",
    "\n",
    "print(vis_matrix.shape)\n",
    "vis_matrix = vis_matrix.reshape([10*28, 5*28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Daten Normalisierung</h2>\n",
    "\n",
    "<p>Solange wie die Eingangsdaten (x) in ein Regression-Model normalisiert sind k&ouml;nnen Lernraten zwischen 0.1 und 1.0 verwendet werden. &Auml;ndert sich der Wertebereich dieser Daten, werden komplett andere Learnraten ben&ouml;tigt. Es ist also Sinnvoll die Daten immer vorher zwischen 0 und 1 zu normalisieren.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.min(), X_train.max())\n",
    "print(X_test.min(), X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Normalize the data.\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>One-Hot-Encoding</h2>\n",
    "\n",
    "<p>Die logistische Regression liefert als Ergebnis Werte zwischen 0 und 1. Damit ist&nbsp;zun&auml;chst keine Multi-Klassen Klassifikation m&ouml;glich, die f&uuml;r das MNIST Beispiel aber notwendig ist. Deswegen m&uuml;ssen die Zieldaten (y) mit dem One-vs-All Ansatz oder auch One-Hot-Encoding genannt transformiert werden. Das Ergebnis sind&nbsp;y_train und y_test Matrizen im Shape von 60000x10 bzw. 10000x10.&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Apply One-Hot-Encoding on the y-Data\n",
    "y_train = np.eye(10)[y_train.astype(int)]\n",
    "y_test = np.eye(10)[y_test.astype(int)]\n",
    "\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Training</h2>\n",
    "\n",
    "<p>Nutzen sie die logistischen Regression um ein Model zu trainieren welches f&uuml;r ein 28x28 Pixel Bild die darauf befindliche Ziffer vorhersagt. Notieren sie sich den Trainingsfehler bez&uuml;glich der Trainings- und Testdaten, sowie die Vorhersagegenautigkeit mit Hilfe der Testdaten. Die Regression liefert f&uuml;r alle eingehenden Daten jeweils einen Vektor mit 10 Dimensionen. Die Dimension die den h&ouml;chsten Wert liefert, gilt als Gewinner und wird verglichen mit dem Gewinner des One-Hot-Vektors. Auf diese Weise l&auml;sst sich ermitteln wie h&auml;ufig die Regression richtig lag (Vorhersagegenautigkeit).</p>\n",
    "\n",
    "<p>Wird die Differenz zwischen den beiden Vektoren ermittelt, k&ouml;nnen mit Hilfe der partiellen Ableitung die Gewichts-/Theta-Werte angepasst werden, sodass in Zukunft die Differenz kleiner ausf&auml;llt. Au&szlig;erdem ist es m&ouml;glich den Mean-Squared-Error der Trainingsdaten anhand dieser Differenz auszurechnen (Trainingsfehler).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(y_pred, y_train):\n",
    "    cost_train = 1/2 * np.sum(np.square(y_pred - y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_cost(y_pred, y, m):\n",
    "    return (- 1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * (np.log(1 - y_pred)))  # compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_error(y_pred, y):\n",
    "    return 100 - np.mean(np.abs(y_pred - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build a logist regression model, using the sigmoid as the activation function and the MSE as the loss function.\n",
    "\n",
    "# pixel count\n",
    "img_size_flat = 28 * 28\n",
    "\n",
    "# num of classes\n",
    "num_classes = 10\n",
    "\n",
    "# initial theta-values\n",
    "# weights = np.random.randn(img_size_flat, num_classes)\n",
    "weights = np.random.randn(img_size_flat, num_classes).astype(np.float32) * np.sqrt(2.0/(784))\n",
    "\n",
    "# learn rate\n",
    "m_train = len(X_train)\n",
    "m_test = len(X_test)\n",
    "alpha = 0.5/ m_train\n",
    "\n",
    "# predictions per iteration\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "\n",
    "# train for 30 iterations\n",
    "for i in range(60):\n",
    "    \n",
    "    # prediction\n",
    "    y_pred = sigmoid(np.dot(X_train, weights))\n",
    "    cost_train = log_cost(y_pred, y_train, m_train)\n",
    "    train_costs.append(cost_train)\n",
    "    \n",
    "    # derivative\n",
    "    d_weights = np.dot(X_train.T, y_pred - y_train)\n",
    "\n",
    "    # update weights\n",
    "    weights = weights - alpha * d_weights\n",
    "    \n",
    "    # print error\n",
    "    print(\"[{:2d}] cost: {:.3f}\".format(i, cost_train))\n",
    "    \n",
    "    # test\n",
    "    y_pred_test = sigmoid(np.dot(X_test, weights))\n",
    "    cost_test = log_cost(y_pred_test, y_test, m_test)\n",
    "    test_costs.append(cost_test)\n",
    "\n",
    "y_pred = sigmoid(np.dot(X_test, weights))\n",
    "y_pred_max = (y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n",
    "\n",
    "print('Test Accuracy: {:.2f}%'.format(100 - np.mean(np.abs(y_pred_max - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Plotten der Trainingsfehler und Testgenauigkeit</h2>\n",
    "\n",
    "<p>Erzeugen sie einen Plot der den Trainings- und den Testfehler in Abh&auml;ngigkeit der Trainingsiteration in einem Diagramm anzeigt.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(test_costs, label='Test Error')\n",
    "plt.plot(train_costs, label='Train Error')\n",
    "plt.legend()\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"Learning rate: {:.10f}\".format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Visualisierung der Weights</h2>\n",
    "\n",
    "<p>Im folgenden wird die Gesichtsmatrix visualisiert. Dadurch soll ersichtlich werden, was die Machine gelernt hat. Je nach Fehlerfunktion und Trainingsalgorithmus sind die entstehenen Bilder mehr oder weniger aufschlussreich für uns Menschen.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights(w):\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "\n",
    "    # Create figure with 3x4 sub-plots,\n",
    "    # where the last 2 sub-plots are unused.\n",
    "    fig, axes = plt.subplots(3, 4)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only use the weights for the first 10 sub-plots.\n",
    "        if i<10:\n",
    "            # Get the weights for the i'th digit and reshape it.\n",
    "            # Note that w.shape == (img_size_flat, 10)\n",
    "            image = w[:, i].reshape((28,28))\n",
    "\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"Weights: {0}\".format(i))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(image, vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "\n",
    "        # Remove ticks from each sub-plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<h2>Abgabe</h2>\n",
    "\n",
    "<p>Das von ihnen erstellte Notebook muss sp&auml;testens bis zum 10. Dezember 2017 um 23:59 UTC+1 per E-Mail an&nbsp;<a href=\"mailto:hezel@htw-berlin.de\" target=\"_blank\">hezel@htw-berlin.de</a>&nbsp;eingesendet werden. Verwenden sie als Betreff bitte &quot;CV1718 &Uuml;bung2 &lt;NAME&gt;&quot; und als Notebook Name &quot;CV1718_Ue2_LogisticRegression_MNIST_NAME.ipynb&quot;. Bevor sie mir eine Mail schicken, entfernen sie bitte &uuml;ber &quot;Kernel&quot; -&gt; &quot;Restart and Clear Output&quot; s&auml;mtlichen von Python erstellten Inhalt und speichern anschlie&szlig;end das Notebook &quot;File&quot; -&gt; &quot;Save and Checkpoint&quot;.</p>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
