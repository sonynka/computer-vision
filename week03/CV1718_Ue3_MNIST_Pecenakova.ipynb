{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistische Regression mit Tensorflow anhand der MNIST Ziffern</h1>\n",
    "\n",
    "<p>Bisher haben wir mit numpy die Modelle und deren Ableitungen manuell programmmiert. Gute Machine Learning Frameworks können uns einiges an Arbeit abnehmen. Dank automatischer Differenzierung müssen wir uns zum Beispiel keine Gedanken mehr machen, wie die Ableitung einer Funktion aussieht. Es reicht aus, wenn wir den sogenannten Vorwärtspass bis hin zur Fehlerfunktion hinschreiben und den Rest dem Framework überlassen. In den nächsten Übungen wollen wir mit der Bibliothek Tensorflow arbeiten, welche sowohl auf der CPU als auch auf der GPU läuft. Es wird empfohlen, das Framework mit Grafikkartenunterstützung zu installieren. Voraussetzung ist eine NVIDIA-Grafikkarte, die im Labor vorzufinden ist.</p>\n",
    "\n",
    "<p>Dieses Jupyter Notebook steht wieder zum <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung3/Tensorflow_Regression_MNIST_Vorlage.ipynb\" target=\"_blank\">Download</a> bereit.</p>\n",
    "\n",
    "<hr />\n",
    "<h2>Vorbereitung</h2>\n",
    "\n",
    "<p>Die bisher verwendete numpy Bibliothek arbeitet <strong>imperative </strong>und f&uuml;hrt jede geschrieben Zeile sofort aus. Niedergeschriebene Formeln werden sofort berechnet und die Ergebnisse stehen im Anschluss zur Verf&uuml;gung. <strong>Deklarative </strong>Programmierung hingegen, beschreibt nur was berechnet werden soll, aber nicht wie und wann. Python selbst ist zwar imperativ, aber der Computation Graph von Tensorflow arbeitet deklarativ. Wir unterscheiden deshalb zwischen zwei Phasen bei der Programmierung mit Tensorflow. Zun&auml;chst erstellen wir den Graphen, das was f&uuml;r uns aussieht wie eine imerative Schreibweise ist in Wirklichkeit eine deklarative. Im zweiten Schritt erstellen wir eine Session, die den Graphen ausf&uuml;hrt und uns dessen Ergebnis ausgibt.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    # create a new graph and use it as the default graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # all computation operations are automatically added to the default graph\n",
    "        x = tf.placeholder(tf.float32, shape=[2,3], name='x')\n",
    "        y = tf.subtract(x, 5, name='y')\n",
    "\n",
    "    # x and y are tensor operators and represent variables inside the graph. They do not hold data themself.\n",
    "    # We can grab the data of a variable computed during a session and print it later.\n",
    "    print(x)\n",
    "    print(y)\n",
    "        \n",
    "    # session configuration\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # start a new session with the new graph\n",
    "    with tf.Session(graph=graph, config=config) as session:  \n",
    "\n",
    "        # construct a subgraph to compute the values of y. Use the provided x data.\n",
    "        y_ndarray = session.run(y, feed_dict={x: [[2,3,4],[8,9,8]]}) \n",
    "\n",
    "        # We get a numpy-array for every tensor-operation we specify in the run-function.\n",
    "        print(y_ndarray)\n",
    "        print(type(y_ndarray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Lineare Regression mit Tensorflow anhand von Beispieldaten</h2>\n",
    "\n",
    "<p>Das folgende Beispiel erstellt zun&auml;chst zuf&auml;llige Datenpunkte und versucht dann mit Hilfe der linearen Regression eine gute approximierende Gerade zu finden. Die Daten und Vorgehensweise ist identisch zu <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung1/#LinearRegression_1D_Vorlage.ipynb\">&Uuml;bung 1</a>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# enable interactive plots\n",
    "%matplotlib notebook\n",
    "\n",
    "def make_plot(x, y, prediction): \n",
    "    fig = plt.figure(figsize = (8,6))   \n",
    "    \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.axis([-2, 2, 0.1, 0.6])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "    dots, = ax.plot(x, y, 'ro')   \n",
    "    line, = ax.plot(x, prediction)    \n",
    "    return fig, dots, line\n",
    "\n",
    "def show_plot(x, y, prediction): \n",
    "    fig, dots, line = make_plot(x, y, prediction)\n",
    "    fig.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) \n",
    "num_points = 100\n",
    "\n",
    "# random data array\n",
    "x = np.zeros([num_points, 1], dtype=np.float32) \n",
    "y = np.zeros([num_points, 1], dtype=np.float32)\n",
    "\n",
    "for i in range(num_points):\n",
    "    x[i] = np.random.normal(0.0, 0.55)\n",
    "    y[i] = x[i] * 0.1 + 0.3 + np.random.normal(0.0, 0.03)    \n",
    "    \n",
    "# initiale theta-Werte\n",
    "t0 =  0.4\n",
    "t1 = -0.3 \n",
    "\n",
    "# prediction\n",
    "y_pred = t0 + t1*x\n",
    "    \n",
    "# print results\n",
    "show_plot(x, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lineare Regression mit Tensorflow</h2>\n",
    "<p>Weiter oben wurde kurz gezeigt, wie Tensorflow generell funktioniert. Im folgenden wurde das Beispiel erweitert und eine lineare Regression implementiert. 50 iterationen lang wird immer wieder die Kostenberechnungs- und die Gewichtsupdatefunktion aufgerufen.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn rate \n",
    "learning_rate = 0.1\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    # computation graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # input data, infer shape at computation time\n",
    "        x_input = tf.placeholder(tf.float32, name='x')\n",
    "        y_input = tf.placeholder(tf.float32, name='y')\n",
    "\n",
    "        # weights and bias initialization following Xavier Glorot and Yoshua Bengio suggestions\n",
    "        xavier = tf.contrib.layers.xavier_initializer(uniform=True, seed=7, dtype=tf.float32)\n",
    "        weights = tf.get_variable(shape=[1, 1], dtype=tf.float32, initializer=xavier, name='weights')\n",
    "        bias = tf.get_variable(shape=[1], dtype=tf.float32, initializer=xavier, name='bias')\n",
    "\n",
    "        # prediction (x * W + b)\n",
    "        prediction = tf.add(bias, tf.matmul(x_input, weights), name='prediction')\n",
    "\n",
    "        # mean squared error\n",
    "        cost = tf.reduce_mean(tf.square(y_input - prediction), name='cost')\n",
    "\n",
    "        # use gradient descent to derive the cost function and update the variables (weights, bias)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # session configuration\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graph, config=config) as session:  \n",
    "\n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "\n",
    "        # cost per iteration\n",
    "        predictions = []\n",
    "        costs = []\n",
    "\n",
    "        # optimise 50 iterations\n",
    "        for i in range(50):\n",
    "            \n",
    "            # compute cost and update weights using x,y \n",
    "            predictions_out, cost_out, _ = session.run([prediction, cost, optimizer], feed_dict={x_input: x, y_input: y})\n",
    "            predictions.append(predictions_out)\n",
    "            costs.append(cost_out)\n",
    "            \n",
    "        # plot cost function\n",
    "        plt.figure()\n",
    "        plt.plot(costs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ergebnis visualisieren</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Slider\n",
    "\n",
    "fig, dots, line = make_plot(x, y, predictions[0])\n",
    "plt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.15)\n",
    "\n",
    "sliderPos = plt.axes([0.15, 0.03, 0.75, 0.03])\n",
    "iterationSlider = Slider(sliderPos, 'Iteration', 0, len(predictions), valfmt=\"%d\",valinit=0)\n",
    "\n",
    "def update(x):\n",
    "    line.set_ydata(predictions[int(x)])\n",
    "    fig.canvas.draw_idle()\n",
    "iterationSlider.on_changed(update)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<h2>MNIST mit Logistischer Regression in Tensorflow</h2>\n",
    "\n",
    "<p>Implementieren Sie die logistische Regression in Tensorflow, die die Ziffern der MNIST Bilder vorhersagen kann. Die Aufgabe ist identisch zu <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung2/#LogisticRegression_MNIST_Vorlage.ipynb\">&Uuml;bung 2</a>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfileobj\n",
    "from sklearn.datasets.base import get_data_home\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def fetch_mnist(data_home=None):\n",
    "    mnist_alternative_url = \"http://home.htw-berlin.de/~hezel/files/data/mnist-original.mat\"    \n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    data_home = os.path.join(data_home, 'mldata')\n",
    "    if not os.path.exists(data_home):\n",
    "        os.makedirs(data_home)\n",
    "    mnist_save_path = os.path.join(data_home, \"mnist-original.mat\")\n",
    "    if not os.path.exists(mnist_save_path):\n",
    "        print(\"Download MNIST to\",mnist_save_path)\n",
    "        mnist_url = urllib.request.urlopen(mnist_alternative_url)\n",
    "        with open(mnist_save_path, \"wb\") as matlab_file:\n",
    "            copyfileobj(mnist_url, matlab_file)\n",
    "    return fetch_mldata('MNIST original')\n",
    "\n",
    "mnist = fetch_mnist()\n",
    "print(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten normalisieren und in Trainungs- und Testdaten einteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# input and output data\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target\n",
    "\n",
    "# shuffle data\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "# split data\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, train_size=60000, test_size=10000)\n",
    "\n",
    "# normalize\n",
    "X_train = (X_train_all / 255.) \n",
    "X_test = (X_test_all / 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die y-Daten One-Hot kodieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "y_train_hot = enc.fit_transform(np.expand_dims(y_train_all, axis=1)).toarray()\n",
    "y_test_hot = enc.fit_transform(np.expand_dims(y_test_all, axis=1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(y_train_hot.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(y_test_hot.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Training</h2>\n",
    "\n",
    "<p>Überführen Sie die logistischen Regression von Übung 2b in eine Tensorflow-Version. Verwenden Sie Softmax als Aktivierungsfunktion und die Cross-Entropy als Fehlerfunktion. Notieren Sie sich den Trainingsfehler anhand der Trainingsdaten und die Vorhersagegenautigkeit mit Hilfe der Testdaten. Plotten Sie den Fehler und die Genauigkeit, um zu überprüfen, ob Ihr Model funktioniert. Es werden Genauigkeiten von ungefähr 92% erwartet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build a logist regression model, using softmax and cross-entroy function\n",
    "\n",
    "# learn rate \n",
    "learning_rate = 0.3\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    # computation graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # input data, infer shape at computation time\n",
    "        x_input = tf.placeholder(tf.float32, name='x')\n",
    "        y_input = tf.placeholder(tf.float32, name='y')\n",
    "\n",
    "        # weights and bias initialization following Xavier Glorot and Yoshua Bengio suggestions\n",
    "        xavier = tf.contrib.layers.xavier_initializer(uniform=True, seed=7, dtype=tf.float32)\n",
    "        weights = tf.get_variable(shape=[784, 10], dtype=tf.float32, initializer=xavier, name='weights')\n",
    "        bias = tf.get_variable(shape=[1], dtype=tf.float32, initializer=xavier, name='bias')\n",
    "\n",
    "        # prediction (x * W + b)\n",
    "        z = tf.add(bias, tf.matmul(x_input, weights), name='z')\n",
    "\n",
    "        # mean squared error\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=z, name='cost'))\n",
    "#         cost = tf.reduce_mean(tf.square(y_input - prediction), name='cost')\n",
    "\n",
    "        # use gradient descent to derive the cost function and update the variables (weights, bias)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # session configuration\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graph, config=config) as session:  \n",
    "\n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "\n",
    "        # cost per iteration\n",
    "        predictions = []\n",
    "        costs = []\n",
    "\n",
    "        # optimise 50 iterations\n",
    "        for i in range(50):\n",
    "            \n",
    "            # compute cost and update weights using x,y \n",
    "            predictions_out, cost_out, _ = session.run([z, cost, optimizer], feed_dict={x_input: X_train, y_input: y_train_hot})\n",
    "            predictions.append(predictions_out)\n",
    "            costs.append(cost_out)\n",
    "        \n",
    "        # test prediction\n",
    "        pred_test, cost_test = session.run([z, cost], feed_dict={x_input: X_test, y_input: y_test_hot})\n",
    "        \n",
    "        # plot cost function\n",
    "        plt.figure()\n",
    "        plt.plot(costs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_max = (np.argmax(predictions[-1], axis=1))\n",
    "y_train_max = (np.argmax(y_train_hot, axis=1))\n",
    "train_acc = np.mean((pred_train_max == y_train_max))\n",
    "print('Train Accuracy: {:.2f}%'.format(train_acc * 100))\n",
    "\n",
    "pred_test_max = (np.argmax(pred_test, axis=1))\n",
    "y_test_max = (np.argmax(y_test_hot, axis=1))\n",
    "test_acc = np.mean((pred_test_max == y_test_max))\n",
    "print('Test Accuracy: {:.2f}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_test = np.array([[-1, -2, 0, 2],[-2, 1, 3, 2],[-1, 1, 0.5, 0],[1, 0.9, 0, -1.2]])\n",
    "_y_test = np.array([[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0]])\n",
    "\n",
    "print(\"_pred_test \\n\",_pred_test)\n",
    "print(\"_y_test \\n\", _y_test)\n",
    "\n",
    "print(\"My accuracy\")\n",
    "print(\"____________\")\n",
    "_y_pred_max = (_pred_test == _pred_test.max(axis=1)[:,None]).astype(int)\n",
    "print(\"_y_pred_max\\n\", _y_pred_max)\n",
    "diff = _y_pred_max - _y_test\n",
    "print(\"\\n_y_pred_max - _y_test\\n\", diff)\n",
    "print(\"\\nnp.abs(diff)\\n\", np.abs(diff))\n",
    "print(\"\\nnp.mean(np.abs(diff))\\n\", np.mean(np.abs(diff)))\n",
    "\n",
    "print(\"\\n\\nNico accuracy\")\n",
    "print(\"____________\")\n",
    "_y_pred_argmax = _pred_test.argmax(axis=1)\n",
    "_y_test_argmax = _y_test.argmax(axis=1)\n",
    "print(\"\\n_y_pred_argmax\", _y_pred_argmax)\n",
    "print(\"\\n_y_test_argmax\", _y_test_argmax)\n",
    "print(\"==\\n\", _y_pred_argmax == _y_test_argmax)\n",
    "print(\"np.mean(==)\\n\", np.mean(_y_pred_argmax == _y_test_argmax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Neural Network</h2>\n",
    "\n",
    "<p>Laut Yann LeCun's Datenbank <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">http://yann.lecun.com/exdb/mnist/</a> sind Neuronale Netzwerke mit 2-3 Schichten besser als einfache Regressionsmodelle. Erweitern Sie Ihr Model zu einem Neuronalen Netzwerk mit mindestens einem Hidden-Layer. Verwenden Sie für diese Schichten die Sigmoid- oder ReLu-Aktivierungsfunktion. Geben Sie wieder den Trainingsfehler und die Testgenauigkeit in einem Diagramm aus. Zu erwarten sind Genauigkeiten von ca. 95%.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement a neural network with at least one hidden layer. Train it and plot the error and accuracy curves.\n",
    "# TODO Build a logist regression model, using softmax and cross-entroy function\n",
    "\n",
    "# learn rate \n",
    "learning_rate = 0.01\n",
    "num_hidden_1 = 256\n",
    "num_hidden_2 = 64\n",
    "num_outputs = 10\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    # computation graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # input data, infer shape at computation time\n",
    "        x_input = tf.placeholder(tf.float32, name='x')\n",
    "        y_input = tf.placeholder(tf.float32, name='y')\n",
    "\n",
    "        # weights and bias initialization following Xavier Glorot and Yoshua Bengio suggestions\n",
    "        xavier = tf.contrib.layers.xavier_initializer(uniform=True, seed=7, dtype=tf.float32)\n",
    "        weights_1 = tf.get_variable(shape=[784, num_hidden_1], dtype=tf.float32, initializer=xavier, name='weights_1')\n",
    "        bias_1 = tf.get_variable(shape=[1], dtype=tf.float32, initializer=xavier, name='bias_1')\n",
    "\n",
    "        # prediction (x * W + b)\n",
    "        z_1 = tf.add(bias_1, tf.matmul(x_input, weights_1), name='z_1')\n",
    "        a_1 = tf.nn.relu(z_1)\n",
    "        \n",
    "        weights_2 = tf.get_variable(shape=[num_hidden_1, num_outputs], dtype=tf.float32, initializer=xavier, name='weights_2')\n",
    "        bias_2 = tf.get_variable(shape=[1], dtype=tf.float32, initializer=xavier, name='bias_2')\n",
    "        \n",
    "        # prediction (x * W + b)\n",
    "        preds = tf.add(bias_2, tf.matmul(a_1, weights_2), name='z_2')\n",
    "#         a_2 = tf.nn.relu(z_2)\n",
    "        \n",
    "#         weights_3 = tf.get_variable(shape=[num_hidden_2, num_outputs], dtype=tf.float32, initializer=xavier, name='weights_3')\n",
    "#         bias_3 = tf.get_variable(shape=[1], dtype=tf.float32, initializer=xavier, name='bias_3')\n",
    "        \n",
    "#         # prediction (x * W + b)\n",
    "#         preds = tf.add(bias_3, tf.matmul(a_2, weights_3), name='z_3')\n",
    "\n",
    "        # mean squared error\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=preds, name='cost'))\n",
    "#         cost = tf.reduce_mean(tf.square(y_input - prediction), name='cost')\n",
    "\n",
    "        # use gradient descent to derive the cost function and update the variables (weights, bias)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # session configuration\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graph, config=config) as session:  \n",
    "\n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "\n",
    "        # cost per iteration\n",
    "        predictions = []\n",
    "        costs = []\n",
    "\n",
    "        # optimise 50 iterations\n",
    "        for i in range(100):\n",
    "            \n",
    "            # compute cost and update weights using x,y \n",
    "            predictions_out, cost_out, _ = session.run([preds, cost, optimizer], feed_dict={x_input: X_train, y_input: y_train_hot})\n",
    "            predictions.append(predictions_out)\n",
    "            costs.append(cost_out)\n",
    "            print('Iteration [{:2}] Cost: {:.4f}'.format(str(i), cost_out))\n",
    "        \n",
    "        # test prediction\n",
    "        pred_test, cost_test = session.run([preds, cost], feed_dict={x_input: X_test, y_input: y_test_hot})\n",
    "        \n",
    "        # plot cost function\n",
    "        plt.figure()\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "pred_train_max = (np.argmax(predictions[-1], axis=1))\n",
    "y_train_max = (np.argmax(y_train_hot, axis=1))\n",
    "train_acc = np.mean((pred_train_max == y_train_max))\n",
    "print('Train Accuracy: {:.2f}%'.format(train_acc * 100))\n",
    "\n",
    "pred_test_max = (np.argmax(pred_test, axis=1))\n",
    "y_test_max = (np.argmax(y_test_hot, axis=1))\n",
    "test_acc = np.mean((pred_test_max == y_test_max))\n",
    "print('Test Accuracy: {:.2f}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<h2>Abgabe</h2>\n",
    "\n",
    "<p>Das von Ihnen erstellte Notebook muss sp&auml;testens bis zum 31. Dezember 2017 um 23:59 UTC+1 ;) per E-Mail an&nbsp;<a href=\"mailto:hezel@htw-berlin.de\" target=\"_blank\">hezel@htw-berlin.de</a>&nbsp;eingesendet werden. Verwenden Sie als Betreff bitte &quot;CV1718 &Uuml;bung3 &lt;NAME&gt;&quot; und als Notebook Name &quot;CV1718_Ue3_Tensorflow_MNIST_NAME.ipynb&quot;. Bevor Sie mir eine Mail schicken, entfernen sie bitte &uuml;ber &quot;Kernel&quot; -&gt; &quot;Restart and Clear Output&quot; s&auml;mtlichen von Python erstellten Inhalt und speichern anschlie&szlig;end das Notebook &quot;File&quot; -&gt; &quot;Save and Checkpoint&quot;.</p>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
