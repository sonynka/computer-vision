{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CIFAR10 mit Convolutional Neural Network</h1>\n",
    "\n",
    "<p>Die bisherigen Netzwerke waren &uuml;blicherweise in wenigen Sekunden oder einigen Minute trainierbar und mit der richtigen Netzwerk Struktur waren Genauigkeiten von &uuml;ber 90% zu erreichen. Das wird sich in dieser &Uuml;bung &auml;ndern, da wir das CIFAR10 Datenset verwenden werden. Es enth&auml;lt 50k Trainings- und 10k Testbilder mit je 32x32 RGB-Pixeln. Wie der Name schon sagt, besteht es aus 10 Kategorien. Auch wenn es nur wenige Bildtypen beinhaltet, ist es dennoch sehr kompliziert und wird in der Forschung eingesetzt um neue Ideen zu testen. <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130\" target=\"_blank\">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130</a></p>\n",
    "\n",
    "\n",
    "<h3>Vorbereitung</h3>\n",
    "\n",
    "<p>F&uuml;r das neue Datenset wurde der Programmcode in der <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/cvutils.py\" target=\"_blank\">cvutils.py</a> Datei aktualisiert. Kopieren Sie sich den Inhalt oder &uuml;berschreiben Sie ihre aktuelle Version mit der Online-Variante. &Uuml;berf&uuml;hren Sie danach das Convolutional Neural Network der letzten &Uuml;bung in die neue Vorlage und passen Sie eventuell die Netzwerkgr&ouml;&szlig;en an. Trainieren Sie einmalig ein paar Iterationen und notieren Sie sich die Genauigkeit ihres ersten Versuches.</p>\n",
    "\n",
    "<p>Das komplette Notebook steht hier zum&nbsp;<a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/Tensorflow_ConvNet_CIFAR10_Vorlage.ipynb\" target=\"_blank\">download</a>&nbsp;bereit.</p>\n",
    "\n",
    "<h3>Aufgabe</h3>\n",
    "\n",
    "<p>Wird das Netzwerk der letzten Woche &uuml;bernommen, sind Genauigkeiten von knapp 70% zu erwarten. Danach tritt Overfitting ein. Schauen Sie sich dazu die Trainingsfehler und die Genauigkeitskurven an. Ziel der &Uuml;bung ist es, mit einen selbst geschriebenen Neuronalen Netzwerk in so kurzer Zeit wie m&ouml;glich, eine gute Vorhersagegenauigkeit f&uuml;r das neue Daten Set zu erreichen. Ihnen sind keine Grenzen gesetzt, welche Verfahren Sie dabei nutzen. Erstrebenswert sind Genauigkeiten um die 80%. Hier sind ein paar Tipps, wie Sie dies erreichen k&ouml;nnen:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>\n",
    "\t<p><strong>Dropout</strong>: Mit Hilfe von Dropout (<a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/dropout\" target=\"_blank\">tf.layers.dropout</a>) k&ouml;nnen Sie das Overfitting reduzieren.&nbsp;</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Weight Regularization</strong>: Die Filterkernel Regulartoren (von <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/conv2d\" target=\"_blank\">tf.layers.conv2d</a>&nbsp;und <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/dense\" target=\"_blank\">tf.layers.dense</a>) k&ouml;nnen daf&uuml;r sorgen, dass die Gewichtwerte keine extrem gro&szlig;en Werte annehmen.&nbsp;</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Data Augmentation</strong>: Um mehr Variation in den Trainingsdaten zu haben, k&ouml;nnen diese erweitert (augmentiert) werden. Es gibt viele Funktionen um zuf&auml;llige leichte Ver&auml;nderungen an den Bildern (<a href=\"https://www.tensorflow.org/api_docs/python/tf/image\" target=\"_blank\">tf.image</a>) vornehmen zu k&ouml;nnen.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Batch normalization</strong>: Indem das Netzwerk &uuml;berall mit normalisieren Daten (<a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/batch_normalization\" target=\"_blank\">tf.nn.batch_normalization</a>) arbeitet, kann es sich besser auf das Wesentliche (Klassifzieren) konzentieren.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Netzwerkstruktur</strong>: Die Anzahl und Gr&ouml;&szlig;e der Filterkernel in einem ConvNet entscheiden dar&uuml;ber wie viele verschiedene komplexe Bildmuster das Netzwerk erkennen kann. Inspirieren Sie sich daher bei der Netzwerkstruktur von&nbsp;dem Paper <a href=\"https://arxiv.org/pdf/1412.6806.pdf\" target=\"_blank\">&quot;Striving for Simplicity: The all Convolutional Net&quot;</a>.</p>\n",
    "\t</li>\n",
    "</ul>\n",
    "\n",
    "<p>Notieren Sie f&uuml;r Ihr bestes Netzwerk die Genauigkeit und schreiben Sie diese, inklusive dem Netzwerkaufbau in eine PDF Datei. Der strukturelle Aufbau kann in Textform oder visuell festgehalten werden.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cvutils import fetch_cifar10\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download MNIST to /home/s0540607/.keras/datasets\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# load CIFAR10 data set\n",
    "cifar = fetch_cifar10()\n",
    "x_train = cifar.train.data.astype('float32')\n",
    "y_train = cifar.train.target.astype('int64')\n",
    "x_test = cifar.test.data.astype('float32')\n",
    "y_test = cifar.test.target.astype('int64')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DROPOUT = True\n",
    "DROPOUT_KEEP_PROB = 0.8\n",
    "NORM = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optional utils\n",
    "def opt_drop(X, keep_prob=DROPOUT_KEEP_PROB, drop=DROPOUT):\n",
    "    \"\"\"\n",
    "        Optionally implements dropout at a given layer\n",
    "    \"\"\"\n",
    "    if drop:\n",
    "        print('WARNING: Applying dropout...')\n",
    "        return tf.nn.dropout(X, keep_prob=keep_prob)\n",
    "    return X\n",
    "\n",
    "def opt_norm(X, name, norm=NORM):\n",
    "    \"\"\"\n",
    "        Local response normalization, used by Alex\n",
    "        API: https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    if norm:\n",
    "        return tf.nn.lrn(X, name=name)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def batch_norm(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('bn'):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iters:  391\n",
      "num_epochs:  3\n",
      "Padded input shape (?, 36, 36, 3)\n",
      "A1 shape: (?, 36, 36, 64)\n",
      "P1 shape: (?, 18, 18, 64)\n",
      "A2 shape: (?, 18, 18, 128)\n",
      "P2 shape: (?, 9, 9, 128)\n",
      "A3 shape: (?, 9, 9, 256)\n",
      "P3 shape: (?, 5, 5, 256)\n",
      "A3_1 shape: (?, 5, 5, 128)\n",
      "A3 flatten shape (?, 3200)\n",
      "A4 shape: (?, 128)\n",
      "A5 shape: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "kernel_size = 3\n",
    "num_channels = x_train.shape[3] #3\n",
    "# Number of channels by layer\n",
    "n_channels_01 = 64\n",
    "n_channels_02 = 128\n",
    "n_channels_03 = 256\n",
    "n_channels_04 = 128\n",
    "n_channels_05 = num_classes \n",
    "\n",
    "# learn rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "batch_size = 128\n",
    "num_iters = math.ceil(x_train.shape[0] / batch_size)\n",
    "print('num_iters: ', num_iters)\n",
    "num_epochs = 3\n",
    "print('num_epochs: ', num_epochs)\n",
    "\n",
    "# Seed\n",
    "seed = 5\n",
    "img_dim = x_train.shape[1] # 32\n",
    "\n",
    "\n",
    "graphCNN = tf.Graph()\n",
    "with graphCNN.as_default():\n",
    "    #x_input = tf.placeholder(dtype=tf.float32, shape=[None, img_dim*img_dim], name='x')\n",
    "    x_input = tf.placeholder(dtype=tf.float32, shape=[None,img_dim, img_dim, num_channels])\n",
    "    y_input = tf.placeholder(tf.int64, shape=[None, y_train.shape[1]], name='y')\n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "    \n",
    "    #x_in = tf.reshape(x_input, shape=[tf.shape(x_input)[0], img_dim, img_dim, num_channels])\n",
    "    \n",
    "    # Network\n",
    "    # Layer 01: output feature maps: 6 filters X 34x34; https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/pad\n",
    "    paddings = tf.constant([[0, 0,], [2, 2,], [2, 2], [0, 0,]])\n",
    "    padded = tf.pad(x_input, paddings, 'CONSTANT')\n",
    "    print(\"Padded input shape\", padded.shape)\n",
    "\n",
    "    # layer 1: output feature maps: 6 filters X 14x14;\n",
    "#     W1 = tf.get_variable(\"W1\", [kernel_size, kernel_size, num_channels, n_channels_01], \n",
    "#                          initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "#     Z1 = tf.nn.conv2d(padded, W1, strides=[1,1,1,1], padding='SAME', name='conv1'\n",
    "#                       kernel_regularize)\n",
    "    Z1 = tf.layers.conv2d(padded, n_channels_01, [kernel_size, kernel_size], padding='SAME', name='conv1',\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.3), \n",
    "                          kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    \n",
    "    Z1 = batch_norm(Z1, n_channels_01, phase_train=phase_train)\n",
    "    \n",
    "    A1 = opt_drop(X=tf.nn.relu(Z1, name='relu1'))\n",
    "\n",
    "    print(\"A1 shape:\", A1.shape)\n",
    "    P1 = opt_norm(X=tf.nn.max_pool(A1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='max_pool1'),\n",
    "                 name='norm1')\n",
    "    print(\"P1 shape:\", P1.shape)\n",
    "\n",
    "    # Layer 02 Conv: output feature maps: 16 filters X 10x10\n",
    "#     W2 = tf.get_variable(\"W2\", [kernel_size, kernel_size, n_channels_01, n_channels_02], \n",
    "#                          initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "#     Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding='SAME', name='conv2')\n",
    "    Z2 = tf.layers.conv2d(P1, n_channels_02, [kernel_size, kernel_size], padding='SAME', name='conv2',\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.3), \n",
    "                          kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    \n",
    "    Z2 = batch_norm(Z2, n_channels_02, phase_train=phase_train)\n",
    "    \n",
    "    A2  = opt_drop(X=tf.nn.relu(Z2, name='relu2'))\n",
    "    print(\"A2 shape:\", A2.shape)\n",
    "    P2 = opt_norm(X=tf.nn.max_pool(A2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='max_pool2'),\n",
    "                 name='norm2')\n",
    "    print(\"P2 shape:\", P2.shape)\n",
    "    \n",
    "    \n",
    "    # Layer 03 Conv: output feature maps: 120 filters X 5x5\n",
    "#     W3 = tf.get_variable(\"W3\", [kernel_size, kernel_size, n_channels_02, n_channels_03], \n",
    "#                          initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "#     Z3 = tf.nn.conv2d(P2, W3, strides=[1,1,1,1], padding='SAME', name='conv3')\n",
    "    Z3 = tf.layers.conv2d(P2, n_channels_03, [kernel_size, kernel_size], padding='SAME', name='conv3',\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.3), \n",
    "                          kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    \n",
    "    \n",
    "    Z3 = batch_norm(Z3, n_channels_03, phase_train=phase_train)\n",
    "    \n",
    "    A3  = opt_norm(X=opt_drop(tf.nn.relu(Z3, name='relu3')),\n",
    "                  name='norm3')\n",
    "    print(\"A3 shape:\", A3.shape)\n",
    "\n",
    "    F = tf.contrib.layers.flatten(A3)\n",
    "    #F = tf.contrib.layers.flatten(P2)\n",
    "    print('A3 flatten shape', F.shape)\n",
    "    \n",
    "    Z4 = tf.contrib.layers.fully_connected(F, n_channels_04, activation_fn=None)\n",
    "    Z4 = tf.contrib.layers.batch_norm(Z4, center=True, scale=True, is_training=True, scope='bn')\n",
    "    #Z4 = batch_norm(Z4, n_channels_04, phase_train=phase_train)\n",
    "    \n",
    "    A4 = X=opt_drop(tf.nn.relu(Z4))\n",
    "    print(\"A4 shape:\", A4.shape)\n",
    "\n",
    "    Z5 = tf.contrib.layers.fully_connected(A4, n_channels_05, activation_fn=None)\n",
    "    #Z5 = tf.contrib.layers.fully_connected(F, n_channels_05, activation_fn=None)\n",
    "    print(\"A5 shape:\", Z5.shape)\n",
    "\n",
    "    prediction = Z5\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=prediction))\n",
    "    \n",
    "    # compute trainings error\n",
    "    cost = tf.losses.sparse_softmax_cross_entropy(labels=y_input, logits=prediction)\n",
    "    \n",
    "    # adding exponential decay to learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-3\n",
    "    end_learning_rate = 5e-3\n",
    "    decay_steps = 10000\n",
    "\n",
    "    learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "                                              decay_steps, end_learning_rate,\n",
    "                                              power=0.5)\n",
    "\n",
    "    exp_learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                   100000, 0.96, staircase=True)\n",
    "    \n",
    "    # use the Adam optimizer to derive the cost function and update the weights\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # accuracy for multiple batches\n",
    "    acc, update_acc = tf.metrics.accuracy(labels=y_input, predictions=tf.argmax(prediction, axis=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error [0/0] 2.92713\n",
      "Test accuracy  0.194\n",
      "Train error [0/500] 1.05036\n",
      "Test accuracy  0.42515\n",
      "Train error [1/0] 0.733456\n",
      "Test accuracy  0.514533\n",
      "Train error [1/500] 0.779862\n",
      "Test accuracy  0.5681\n",
      "Train error [2/0] 0.771092\n",
      "Test accuracy  0.60218\n",
      "Train error [2/500] 0.578959\n",
      "Test accuracy  0.628867\n",
      "Train error [3/0] 0.449598\n",
      "Test accuracy  0.648129\n",
      "Train error [3/500] 0.416569\n",
      "Test accuracy  0.663787\n",
      "Train error [4/0] 0.498659\n",
      "Test accuracy  0.676133\n",
      "Train error [4/500] 0.217038\n",
      "Test accuracy  0.68604\n",
      "Train error [5/0] 0.283774\n",
      "Test accuracy  0.694109\n",
      "Train error [5/500] 0.228783\n",
      "Test accuracy  0.702175\n",
      "Train error [6/0] 0.279478\n",
      "Test accuracy  0.709154\n",
      "Train error [6/500] 0.267573\n",
      "Test accuracy  0.715514\n",
      "Train error [7/0] 0.324472\n",
      "Test accuracy  0.720833\n",
      "Train error [7/500] 0.430671\n",
      "Test accuracy  0.725469\n",
      "Train error [8/0] 0.0978293\n",
      "Test accuracy  0.729694\n",
      "Train error [8/500] 0.180186\n",
      "Test accuracy  0.732967\n",
      "Train error [9/0] 0.0811786\n",
      "Test accuracy  0.736442\n",
      "Train error [9/500] 0.108288\n",
      "Test accuracy  0.739605\n",
      "Train error [10/0] 0.102651\n",
      "Test accuracy  0.742486\n",
      "Train error [10/500] 0.0880293\n",
      "Test accuracy  0.745205\n",
      "Train error [11/0] 0.13986\n",
      "Test accuracy  0.747474\n",
      "Train error [11/500] 0.0619292\n",
      "Test accuracy  0.749987\n",
      "Train error [12/0] 0.0781489\n",
      "Test accuracy  0.751816\n",
      "Train error [12/500] 0.0852119\n",
      "Test accuracy  0.753792\n",
      "Train error [13/0] 0.0384469\n",
      "Test accuracy  0.755256\n",
      "Train error [13/500] 0.0259942\n",
      "Test accuracy  0.756846\n",
      "Train error [14/0] 0.0870284\n",
      "Test accuracy  0.758434\n",
      "Train error [14/500] 0.0755562\n",
      "Test accuracy  0.759947\n",
      "Test accuracy  0.761323\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "# learn rate\n",
    "learning_rate = 0.03\n",
    "batch_size = 64\n",
    "num_iters = math.ceil(x_train.shape[0] / batch_size)\n",
    "\n",
    "# session configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graphCNN, config=config) as session:  \n",
    "    \n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "        \n",
    "        # which nodes to fetch from the computation graph\n",
    "        fetch_train_nodes = {\n",
    "            'cost' : cost,\n",
    "            'optimizer' : optimizer \n",
    "        }\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # shuffle data\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train = x_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            for i in range(num_iters):\n",
    "                X_batch = x_train[i * batch_size:(i + 1) * batch_size]\n",
    "                y_batch = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                output_batch = session.run(fetch_train_nodes, feed_dict={x_input: X_batch, y_input: y_batch,\n",
    "                      phase_train: True})\n",
    "                if(i % 100) == 0:\n",
    "#                     print('Elapsed time (min.s): ', math.ceil((time.time() - start)/60))\n",
    "                    train_costs.append(output_batch[\"cost\"])\n",
    "                \n",
    "                if(i % 500) == 0:\n",
    "                    print(\"Train error [{}/{}]\".format(epoch, i), output_batch[\"cost\"])\n",
    "                    test_acc = session.run(update_acc, feed_dict={x_input: x_test, y_input: y_test, \n",
    "                      phase_train: False})\n",
    "                    test_costs.append(test_acc)\n",
    "                    print(\"Test accuracy \", test_acc)\n",
    "            \n",
    "        # check against test set\n",
    "        print(\"Test accuracy \", session.run(update_acc, feed_dict={x_input: x_test, y_input: y_test,\n",
    "                      phase_train: False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Abgabe</h2>\n",
    "\n",
    "<p>Das von Ihnen erstellte Notebook und die PDF Datei m&uuml;ssen sp&auml;testens bis zum 21. Januar 2018 um 23:59 UTC+1 ;) per E-Mail an&nbsp;<a href=\"mailto:hezel@htw-berlin.de\" target=\"_blank\">hezel@htw-berlin.de</a>&nbsp;eingesendet werden. Verwenden Sie als Betreff bitte &quot;CV1718 &Uuml;bung4 &lt;NAME&gt;&quot; und als Notebook Name &quot;CV1718_Ue4_Tensorflow_ConvNet_CIFAR_NAME.ipynb&quot; sowie &quot;CV1718_Ue4_Tensorflow_ConvNet_CIFAR_NAME.pdf&quot; f&uuml;r die PDF. Bevor Sie mir eine Mail schicken, entfernen Sie bitte &uuml;ber &quot;Kernel&quot; -&gt; &quot;Restart and Clear Output&quot; s&auml;mtlichen von Python erstellten Inhalt und speichern anschlie&szlig;end das Notebook &quot;File&quot; -&gt; &quot;Save and Checkpoint&quot;.</p>\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
