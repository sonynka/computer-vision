{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutional Neural Networks anhand von MNIST</h1>\n",
    "\n",
    "<p>Das neuronale Netzwerk der letzten &Uuml;bung brauchte nur einige hundert Megabyte Arbeitsspeicher. Die 60000 MNIST Bilder mit ihren 784 Pixeln, die Gewichtsmatrizen und alle Zwischenergebnisse, die im Netzwerk beim Vorw&auml;rtspass berechnet wurden, sind keine 350MB gro&szlig;:<br />\n",
    "(60000&lowast; 784 + 784&lowast; 300 + 60000 &lowast; 300 + 300 &lowast; 10 + 60000 &lowast; 10) &lowast; 4 = 335MB <br />(MNISTData + Weights1 + Intermediate + Weight2 + Predictions) &lowast; Float32</p>\n",
    "\n",
    "<p>Dieser Verbrauch steigt rasant an, wenn Konvolutionsfilter ins Spiel kommen. Werden die Eingangsdaten mit 10 Filtern beliebiger Gr&ouml;&szlig;e (z.b. 3x3) gefalten, sind die ausgehenden Daten 10 mal so gro&szlig;:<br />\n",
    "(60000&lowast;784+10&lowast;3&lowast;3+60000&lowast;10&lowast;784)&lowast;4=2GB</p>\n",
    "\n",
    "<p>Um das zu verhindern, sollten in Zukunft nicht mehr alle Daten auf einmal im Netzwerk verarbeitet werden. Stattdessen werden Mini-Batches ben&ouml;tigt.</p>\n",
    "\n",
    "<p>Das komplette Notebook steht wieder zum <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/Tensorflow_ConvNet_MNIST_Vorlage.ipynb\">download</a> bereit.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2>Neural Networks mit Mini-Batches</h2>\n",
    "\n",
    "<p>Im folgenden ist ein zweischichtiges neuronales Netzwerk implementiert, welches alle MNIST Ziffern auf einmal verarbeitet. Bauen Sie den Code so um, dass er stattdessen mit Mini-Batches funktioniert. Dabei&nbsp;k&ouml;nnen Sie Numpy verwenden und die Daten als Mini-Batch in den Computation-Graph von Tensorflow geben&nbsp;oder Sie benutzen Tensorflows Batch-Methoden, um die Batches innerhalb eines Graphens zu erzeugen. Die K&ouml;nigsdiziplin sind Tensorflow Esitmators, die die Arbeit des Batchings &uuml;bernehmen, aber viele andere Anforderungen an das Netzwerk haben.&nbsp;Wichtig ist in allen&nbsp;F&auml;llen, dass auch die Testdaten gebatched werden.</p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Numpy</strong>: Es ist m&ouml;glich die Daten einmalig in kleine Batches zu unterteilen und diese dann in zuf&auml;lliger Reihnfolge in den Computation-Graph zu geben. Besser jedoch ist die Variante, bei der erst im letzten Moment ein Batch aus dem gesamten Datensatz extrahiert wird. Der Extraktionsbereich sollte dabei zuf&auml;llig gew&auml;hlt sein.&nbsp;&nbsp;</li>\n",
    "\t<li><strong>Tensorflow Batch</strong>: Sind die&nbsp;Daten klein genug, dass Sie&nbsp;in den Arbeitsspeicher, aber nicht durch das Netzwerk passen. K&ouml;nnen Sie zun&auml;chst&nbsp;komplett in den Graphen geladen werden und von dort in kleine Batches, mit einem <a href=\"http://www.tensorflow.org/api_docs/python/tf/train/slice_input_producer\">Slice Producer</a>, zerlegt werden. Der Slicer braucht einen <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/train/Coordinator\">Coordinator</a>, der das Zerlegen der Daten mithilfe von mehreren Threads koordiniert.</li>\n",
    "\t<li><strong>Tensorflow Estimator</strong>: Innerhalb der High-Level API von Tensorflow gibt es die Möglichkeit, Estimators zu verwenden. Diese übernehmen sämtliche Batching-Arbeiten, verlangen aber bestimmte Eigenschaften vom Computation-Graphen. So m&uuml;ssen Trainings- und Evaluierungsmethoden in einen sogenannten <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\">EstimatorSpec</a> beschrieben werden, um sp&auml;ter mit einen <a href=\"http://www.tensorflow.org/api_docs/python/tf/estimator\">Estimator Model</a> arbeiten zu k&ouml;nnen.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Der Programmcode für den Import der MNIST Zahlen wurde in die <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/cvutils.py\">cvutils.py</a> Datei ausgelagert. Bitte kopieren Sie die Datei in den Ordner, in dem das Notebook läuft. Ansonsten wird der Import \"from cvutils import fetch_mnist\" im nächsten Abschnitt fehlschlagen.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cvutils import fetch_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# input and output data\n",
    "mnist = fetch_mnist()\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "# shuffle data\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "# split data, both sizes are a multiple of 2048\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=59392, test_size=10240)\n",
    "\n",
    "# normalize\n",
    "X_train = (X_train / 255.) \n",
    "X_test = (X_test / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pixel count\n",
    "num_input = 28 * 28\n",
    "\n",
    "# num of classes\n",
    "num_classes = 10\n",
    "\n",
    "# learn rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "batch_size = 32\n",
    "num_iters = math.ceil(X_train.shape[0] / batch_size)\n",
    "\n",
    "# computation graph\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # input data with fix shape to infer shapes of other graph nodes a build time\n",
    "    x_input = tf.placeholder(dtype=tf.float32, shape=[None, num_input], name='x')\n",
    "    y_input = tf.placeholder(tf.int64, shape=[None], name='y')\n",
    "        \n",
    "    # two layer network: 784 -> 300 -> 10\n",
    "    layer1 = tf.layers.dense(inputs=x_input, units=300, activation=tf.nn.relu)\n",
    "    prediction = tf.layers.dense(inputs=layer1, units=num_classes)\n",
    "    \n",
    "    # compute trainings error\n",
    "    cost = tf.losses.sparse_softmax_cross_entropy(labels=y_input, logits=prediction)\n",
    "    \n",
    "    # use the Adam optimizer to derive the cost function and update the weights\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # accuracy for multiple batches\n",
    "    acc, update_acc = tf.metrics.accuracy(labels=y_input, predictions=tf.argmax(prediction, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graph, config=config) as session:  \n",
    "    \n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "        \n",
    "        # which nodes to fetch from the computation graph\n",
    "        fetch_train_nodes = {\n",
    "            'cost' : cost,\n",
    "            'optimizer' : optimizer \n",
    "        }\n",
    "        \n",
    "        for epoch in range(3):\n",
    "            \n",
    "            # shuffle data\n",
    "            permutation = np.random.permutation(X_train.shape[0])\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            for i in range(num_iters):\n",
    "                X_batch = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "                y_batch = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                output_batch = session.run(fetch_train_nodes, feed_dict={x_input: X_batch, y_input: y_batch})\n",
    "                train_costs.append(output_batch[\"cost\"])\n",
    "                if(i % 500) == 0:\n",
    "                    print(\"Train error [{}/{}]\".format(epoch, i), output_batch[\"cost\"])\n",
    "            \n",
    "        # check against test set\n",
    "        print(\"Test accuracy \", session.run(update_acc, feed_dict={x_input: X_test, y_input: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h2>Convolutional Neural Network mit MNIST Ziffern</h2>\n",
    "\n",
    "<p>Nachdem das neuronale Netzwerk mit Mini-Batches arbeitet, k&ouml;nnen die Fully-Connected (Dense) Layer mit Konvolutionsschichten ersetzt werden. Sinnvoll sind z.B. zwei Schichten mit 64 5x5 und 96 3x3 Filterkerneln. Um die Dimensionalit&auml;t der Daten langsam zu reduzieren, k&ouml;nnen entweder Schrittweiten bei den Konvolutionsschichten eingestellt werden oder Pooling angewendet werden. Zum Schluss ist es hilfreich, die hochdimensionalen Daten zu flatten, um sie in Dense Layern auf 10 Dimensionen herunterzubrechen. Berechnen Sie wieder den Trainingsfehler und die Testgenauigkeit. Zu erwarten sind Genauigkeiten von bis zu 99%.&nbsp;</p>\n",
    "\n",
    "<p>Je nachdem welche Tensorflow Version Sie nutzen (mindestens aber Version &gt;= 1.0), sind folgende Methoden hilfreich:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\" target=\"_blank\">tf.nn.max_pool</a> oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/max_pooling2d\" target=\"_blank\">tf.layers.max_pooling2d</a></li>\n",
    "\t<li><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/conv2d\" target=\"_blank\">tf.nn.conv2d</a> oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/conv2d\" target=\"_blank\">tf.layers.conv2d</a></li>\n",
    "\t<li><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/reshape\" target=\"_blank\">tf.reshape</a> oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/layers/flatten\" target=\"_blank\">tf.contrib.layers.flatten</a> oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/flatten\" target=\"_blank\">tf.layers.Flatten</a></li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Optional</strong>: Yann LeCun hat vor fast 20 Jahren das MNIST Datenset herausgebracht und die Convolutionsnetzwerke erfunden. Damals gab es nicht die nötige Rechenleistung um in kurzer Zeit die notwendigen Filterkernel mittels Backpropagation und Gradient Descent zu erlernen. Seine Netzwerke sind daher sehr minimalistisch. Implementieren Sie das <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\">LeNet5</a> Netzwerk nach seinen Vorbild. Padden Sie dazu die&nbsp;Eingangsdaten, damit die Bilder 32x32 Pixel haben und verwenden Sie nur 6, 16 und 120 Filterkernel&nbsp;(je 5x5 Pixel gro&szlig;) f&uuml;r die drei Konvolutionsschichten in LeNet5. Natürlich können Sie auch LeCun's <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\" target=\"_blank\">Stochastic gradient descent</a> nutzen um ihr Netzwerk zu trainieren oder gar den <a href=\"https://arxiv.org/pdf/1412.6980v8.pdf\" target=\"_blank\">Adam Optimizer</a>.</p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p><img alt=\"LeNet5\" src=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/LeNet5.png\" style=\"max-width: 790px; max-height: 100%; width: auto\" /><br />\n",
    "&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded input shape (?, 32, 32, 1)\n",
      "A1 shape: (?, 28, 28, 6)\n",
      "P1 shape: (?, 14, 14, 6)\n",
      "A2 shape: (?, 10, 10, 16)\n",
      "P2 shape: (?, 5, 5, 16)\n",
      "A3 shape: (?, 5, 5, 120)\n",
      "A3 flatten shape (?, 3000)\n",
      "A4 shape: (?, 84)\n",
      "A5 shape: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "# TODO Copy the neural network code and implement a convolutional version. Try to reach an accuracy over 96%.\n",
    "num_classes = 10\n",
    "kernel_size = 5\n",
    "num_channels = 1\n",
    "# Number of channels by layer\n",
    "n_channels_01 = 6\n",
    "n_channels_02 = 16\n",
    "n_channels_03 = 120\n",
    "n_channels_04 = 84\n",
    "n_channels_05 = num_classes \n",
    "\n",
    "# learn rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "batch_size = 32\n",
    "num_iters = math.ceil(X_train.shape[0] / batch_size)\n",
    "\n",
    "# Seed\n",
    "seed = 5\n",
    "img_dim = 28\n",
    "\n",
    "\n",
    "graphCNN = tf.Graph()\n",
    "with graphCNN.as_default():\n",
    "    x_input = tf.placeholder(dtype=tf.float32, shape=[None, img_dim*img_dim], name='x')\n",
    "    y_input = tf.placeholder(tf.int64, shape=[None], name='y')\n",
    "    \n",
    "    x_in = tf.reshape(x_input, shape=[tf.shape(x_input)[0], img_dim, img_dim, 1])\n",
    "    \n",
    "    # Network\n",
    "    # Layer 01: output feature maps: 6 filters X 28x28; https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/pad\n",
    "    paddings = tf.constant([[0, 0,], [2, 2,], [2, 2], [0, 0,]])\n",
    "    padded = tf.pad(x_in, paddings, 'CONSTANT')\n",
    "    print(\"Padded input shape\", padded.shape)\n",
    "\n",
    "    # layer 1: output feature maps: 6 filters X 14x14;\n",
    "    W1 = tf.get_variable(\"W1\", [kernel_size, kernel_size, 1, n_channels_01], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    Z1 = tf.nn.conv2d(padded, W1, strides=[1,1,1,1], padding='VALID', name='conv1')\n",
    "    A1  = tf.nn.relu(Z1, name='relu1')\n",
    "    print(\"A1 shape:\", A1.shape)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='max_pool1')\n",
    "    print(\"P1 shape:\", P1.shape)\n",
    "    # 14x14\n",
    "\n",
    "    # Layer 02 Conv: output feature maps: 16 filters X 10x10\n",
    "    W2 = tf.get_variable(\"W2\", [kernel_size, kernel_size, n_channels_01, n_channels_02], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding='VALID', name='conv2')\n",
    "    A2  = tf.nn.relu(Z2, name='relu2')\n",
    "    print(\"A2 shape:\", A2.shape)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID', name='max_pool2')\n",
    "    print(\"P2 shape:\", P2.shape)\n",
    "    # 14x14\n",
    "\n",
    "    # Layer 03 Conv: output feature maps: 120 filters X 5x5\n",
    "    W3 = tf.get_variable(\"W3\", [kernel_size, kernel_size, n_channels_02, n_channels_03], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    Z3 = tf.nn.conv2d(P2, W3, strides=[1,1,1,1], padding='SAME', name='conv3')\n",
    "    A3  = tf.nn.relu(Z3, name='relu3')\n",
    "    print(\"A3 shape:\", A3.shape)\n",
    "\n",
    "    F = tf.contrib.layers.flatten(A3)\n",
    "    print('A3 flatten shape', F.shape)\n",
    "    \n",
    "    Z4 = tf.contrib.layers.fully_connected(F, n_channels_04, activation_fn=None)\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    print(\"A4 shape:\", A4.shape)\n",
    "\n",
    "    Z5 = tf.contrib.layers.fully_connected(A4, n_channels_05, activation_fn=None)\n",
    "    print(\"A5 shape:\", Z5.shape)\n",
    "\n",
    "    prediction = Z5\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=prediction))\n",
    "    \n",
    "    # compute trainings error\n",
    "    cost = tf.losses.sparse_softmax_cross_entropy(labels=y_input, logits=prediction)\n",
    "    \n",
    "    # use the Adam optimizer to derive the cost function and update the weights\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # accuracy for multiple batches\n",
    "    acc = tf.contrib.metrics.accuracy(labels=y_input, predictions=tf.argmax(prediction, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error [0/0] train: 2.3007 test accuracy: 0.1562%\n",
      "Error [0/100] train: 0.0679 test accuracy: 0.8438%\n",
      "Error [0/200] train: 0.0587 test accuracy: 0.9688%\n",
      "Error [0/300] train: 0.4822 test accuracy: 0.9062%\n",
      "Error [0/400] train: 0.0285 test accuracy: 1.0000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3fdd8b2fe589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_train_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonynka/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonynka/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonynka/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/sonynka/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonynka/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# session configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graphCNN, config=config) as session:  \n",
    "    \n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "        \n",
    "        # which nodes to fetch from the computation graph\n",
    "        fetch_train_nodes = {\n",
    "            'cost' : cost,\n",
    "            'optimizer' : optimizer \n",
    "        }\n",
    "        \n",
    "        for epoch in range(1):\n",
    "            \n",
    "            # shuffle data\n",
    "            permutation = np.random.permutation(X_train.shape[0])\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            \n",
    "            for i in range(num_iters):\n",
    "                X_batch = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "                y_batch = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                output_batch = session.run(fetch_train_nodes, feed_dict={x_input: X_batch, y_input: y_batch})\n",
    "                if(i % 100) == 0:\n",
    "                    \n",
    "                    train_acc = session.run(acc, feed_dict={x_input: X_batch, y_input:y_batch})\n",
    "                    train_cost = output_batch[\"cost\"]\n",
    "                    train_costs.append(train_cost)\n",
    "                    \n",
    "                    i_test = np.random.randint(X_test.shape[0], size=batch_size)\n",
    "                    X_batch_test = X_test[i_test]\n",
    "                    y_batch_test = y_test[i_test]\n",
    "                    test_acc = session.run(acc, feed_dict={x_input: X_batch_test, y_input: y_batch_test})\n",
    "                    test_costs.append(test_acc)\n",
    "            \n",
    "                    print(\"Error [{}/{}] train: {:.4f} test accuracy: {:.4f}%\".format(epoch, i, train_cost, test_acc))\n",
    "    \n",
    "            \n",
    "        # check against test set\n",
    "        print(\"Test accuracy \", session.run(acc, feed_dict={x_input: X_test, y_input: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(test_costs, label='Test Error')\n",
    "plt.plot(train_costs, label='Train Error')\n",
    "plt.legend()\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"Learning rate: {:.5f}\".format(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<h2>Abgabe</h2>\n",
    "\n",
    "<p>Das von Ihnen erstellte Notebook muss sp&auml;testens bis zum 14. Januar 2018 um 23:59 UTC+1 ;) per E-Mail an&nbsp;<a href=\"mailto:hezel@htw-berlin.de\" target=\"_blank\">hezel@htw-berlin.de</a>&nbsp;eingesendet werden. Verwenden Sie als Betreff bitte &quot;CV1718 &Uuml;bung4 &lt;NAME&gt;&quot; und als Notebook Name &quot;CV1718_Ue4_Tensorflow_ConvNet_MNIST_NAME.ipynb&quot;. Bevor Sie mir eine Mail schicken, entfernen Sie bitte &uuml;ber &quot;Kernel&quot; -&gt; &quot;Restart and Clear Output&quot; s&auml;mtlichen von Python erstellten Inhalt und speichern anschlie&szlig;end das Notebook &quot;File&quot; -&gt; &quot;Save and Checkpoint&quot;.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
