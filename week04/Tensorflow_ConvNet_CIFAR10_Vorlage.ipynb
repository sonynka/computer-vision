{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CIFAR10 mit Convolutional Neural Network</h1>\n",
    "\n",
    "<p>Die bisherigen Netzwerke waren &uuml;blicherweise in wenigen Sekunden oder einigen Minute trainierbar und mit der richtigen Netzwerk Struktur waren Genauigkeiten von &uuml;ber 90% zu erreichen. Das wird sich in dieser &Uuml;bung &auml;ndern, da wir das CIFAR10 Datenset verwenden werden. Es enth&auml;lt 50k Trainings- und 10k Testbilder mit je 32x32 RGB-Pixeln. Wie der Name schon sagt, besteht es aus 10 Kategorien. Auch wenn es nur wenige Bildtypen beinhaltet, ist es dennoch sehr kompliziert und wird in der Forschung eingesetzt um neue Ideen zu testen. <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130\" target=\"_blank\">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130</a></p>\n",
    "\n",
    "\n",
    "<h3>Vorbereitung</h3>\n",
    "\n",
    "<p>F&uuml;r das neue Datenset wurde der Programmcode in der <a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/cvutils.py\" target=\"_blank\">cvutils.py</a> Datei aktualisiert. Kopieren Sie sich den Inhalt oder &uuml;berschreiben Sie ihre aktuelle Version mit der Online-Variante. &Uuml;berf&uuml;hren Sie danach das Convolutional Neural Network der letzten &Uuml;bung in die neue Vorlage und passen Sie eventuell die Netzwerkgr&ouml;&szlig;en an. Trainieren Sie einmalig ein paar Iterationen und notieren Sie sich die Genauigkeit ihres ersten Versuches.</p>\n",
    "\n",
    "<p>Das komplette Notebook steht hier zum&nbsp;<a href=\"http://home.htw-berlin.de/~hezel/computervision/WS1718/uebung4/Tensorflow_ConvNet_CIFAR10_Vorlage.ipynb\" target=\"_blank\">download</a>&nbsp;bereit.</p>\n",
    "\n",
    "<h3>Aufgabe</h3>\n",
    "\n",
    "<p>Wird das Netzwerk der letzten Woche &uuml;bernommen, sind Genauigkeiten von knapp 70% zu erwarten. Danach tritt Overfitting ein. Schauen Sie sich dazu die Trainingsfehler und die Genauigkeitskurven an. Ziel der &Uuml;bung ist es, mit einen selbst geschriebenen Neuronalen Netzwerk in so kurzer Zeit wie m&ouml;glich, eine gute Vorhersagegenauigkeit f&uuml;r das neue Daten Set zu erreichen. Ihnen sind keine Grenzen gesetzt, welche Verfahren Sie dabei nutzen. Erstrebenswert sind Genauigkeiten um die 80%. Hier sind ein paar Tipps, wie Sie dies erreichen k&ouml;nnen:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>\n",
    "\t<p><strong>Dropout</strong>: Mit Hilfe von Dropout (<a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/dropout\" target=\"_blank\">tf.layers.dropout</a>) k&ouml;nnen Sie das Overfitting reduzieren.&nbsp;</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Weight Regularization</strong>: Die Filterkernel Regulartoren (von <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/conv2d\" target=\"_blank\">tf.layers.conv2d</a>&nbsp;und <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/dense\" target=\"_blank\">tf.layers.dense</a>) k&ouml;nnen daf&uuml;r sorgen, dass die Gewichtwerte keine extrem gro&szlig;en Werte annehmen.&nbsp;</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Data Augmentation</strong>: Um mehr Variation in den Trainingsdaten zu haben, k&ouml;nnen diese erweitert (augmentiert) werden. Es gibt viele Funktionen um zuf&auml;llige leichte Ver&auml;nderungen an den Bildern (<a href=\"https://www.tensorflow.org/api_docs/python/tf/image\" target=\"_blank\">tf.image</a>) vornehmen zu k&ouml;nnen.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Batch normalization</strong>: Indem das Netzwerk &uuml;berall mit normalisieren Daten (<a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/batch_normalization\" target=\"_blank\">tf.nn.batch_normalization</a>) arbeitet, kann es sich besser auf das Wesentliche (Klassifzieren) konzentieren.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Netzwerkstruktur</strong>: Die Anzahl und Gr&ouml;&szlig;e der Filterkernel in einem ConvNet entscheiden dar&uuml;ber wie viele verschiedene komplexe Bildmuster das Netzwerk erkennen kann. Inspirieren Sie sich daher bei der Netzwerkstruktur von&nbsp;dem Paper <a href=\"https://arxiv.org/pdf/1412.6806.pdf\" target=\"_blank\">&quot;Striving for Simplicity: The all Convolutional Net&quot;</a>.</p>\n",
    "\t</li>\n",
    "</ul>\n",
    "\n",
    "<p>Notieren Sie f&uuml;r Ihr bestes Netzwerk die Genauigkeit und schreiben Sie diese, inklusive dem Netzwerkaufbau in eine PDF Datei. Der strukturelle Aufbau kann in Textform oder visuell festgehalten werden.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download MNIST to /Users/sonynka/.keras/datasets\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cvutils import fetch_cifar10\n",
    "import math\n",
    "\n",
    "# load CIFAR10 data set\n",
    "cifar = fetch_cifar10()\n",
    "X_train = cifar.train.data.astype('float32')\n",
    "y_train = cifar.train.target.astype('int64')\n",
    "X_test = cifar.test.data.astype('float32')\n",
    "y_test = cifar.test.target.astype('int64')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_train = (X_train / 255.) \n",
    "X_test = (X_test / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (?, 32, 32, 3)\n",
      "A1 shape: (?, 28, 28, 6)\n",
      "P1 shape: (?, 14, 14, 6)\n",
      "A2 shape: (?, 10, 10, 16)\n",
      "P2 shape: (?, 5, 5, 16)\n",
      "A3 shape: (?, 5, 5, 120)\n",
      "A3 flatten shape (?, 3000)\n",
      "A4 shape: (?, 84)\n",
      "A5 shape: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "# TODO Copy the neural network code and implement a convolutional version. Try to reach an accuracy over 96%.\n",
    "num_classes = 10\n",
    "kernel_size = 5\n",
    "num_channels = 1\n",
    "# Number of channels by layer\n",
    "n_channels_01 = 6\n",
    "n_channels_02 = 16\n",
    "n_channels_03 = 120\n",
    "n_channels_04 = 84\n",
    "n_channels_05 = num_classes \n",
    "\n",
    "# learn rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "batch_size = 32\n",
    "num_iters = math.ceil(X_train.shape[0] / batch_size)\n",
    "\n",
    "# Seed\n",
    "seed = 5\n",
    "img_dim = 32\n",
    "\n",
    "\n",
    "graphCNN = tf.Graph()\n",
    "with graphCNN.as_default():\n",
    "    x_input = tf.placeholder(dtype=tf.float32, shape=[None, img_dim, img_dim, 3], name='x')\n",
    "    y_input = tf.placeholder(tf.int64, shape=[None, 1], name='y')\n",
    "    print('Input shape', x_input.shape)\n",
    "\n",
    "    # layer 1: output feature maps: 6 filters X 14x14;\n",
    "    W1 = tf.get_variable(\"W1\", [kernel_size, kernel_size, 3, n_channels_01], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    Z1 = tf.nn.conv2d(x_input, W1, strides=[1,1,1,1], padding='VALID', name='conv1')\n",
    "    A1  = tf.nn.relu(Z1, name='relu1')\n",
    "    print(\"A1 shape:\", A1.shape)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='max_pool1')\n",
    "    print(\"P1 shape:\", P1.shape)\n",
    "    # 14x14\n",
    "\n",
    "    # Layer 02 Conv: output feature maps: 16 filters X 10x10\n",
    "    W2 = tf.get_variable(\"W2\", [kernel_size, kernel_size, n_channels_01, n_channels_02], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding='VALID', name='conv2')\n",
    "    A2  = tf.nn.relu(Z2, name='relu2')\n",
    "    print(\"A2 shape:\", A2.shape)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID', name='max_pool2')\n",
    "    print(\"P2 shape:\", P2.shape)\n",
    "    # 14x14\n",
    "\n",
    "    # Layer 03 Conv: output feature maps: 120 filters X 5x5\n",
    "    W3 = tf.get_variable(\"W3\", [kernel_size, kernel_size, n_channels_02, n_channels_03], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "    Z3 = tf.nn.conv2d(P2, W3, strides=[1,1,1,1], padding='SAME', name='conv3')\n",
    "    A3  = tf.nn.relu(Z3, name='relu3')\n",
    "    print(\"A3 shape:\", A3.shape)\n",
    "\n",
    "    F = tf.contrib.layers.flatten(A3)\n",
    "    print('A3 flatten shape', F.shape)\n",
    "    \n",
    "    Z4 = tf.contrib.layers.fully_connected(F, n_channels_04, activation_fn=None)\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    print(\"A4 shape:\", A4.shape)\n",
    "\n",
    "    Z5 = tf.contrib.layers.fully_connected(A4, n_channels_05, activation_fn=None)\n",
    "    print(\"A5 shape:\", Z5.shape)\n",
    "\n",
    "    prediction = Z5\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=prediction))\n",
    "    \n",
    "    # compute trainings error\n",
    "    cost = tf.losses.sparse_softmax_cross_entropy(labels=y_input, logits=prediction)\n",
    "    \n",
    "    # use the Adam optimizer to derive the cost function and update the weights\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # accuracy for multiple batches\n",
    "    acc = tf.contrib.metrics.accuracy(labels=y_input, predictions=tf.argmax(prediction, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error [0/0] train: 2.3041 test: 3.1120\n",
      "Train accuracy 0.1426%\n",
      "Test accuracy 0.1074%\n",
      "Error [0/100] train: 2.2947 test: 2.2533\n",
      "Error [0/200] train: 2.1761 test: 2.1079\n",
      "Error [0/300] train: 2.3913 test: 1.9258\n",
      "Error [0/400] train: 1.6969 test: 1.9282\n",
      "Error [0/500] train: 1.4547 test: 1.8728\n",
      "Train accuracy 0.1494%\n",
      "Test accuracy 0.1152%\n",
      "Error [0/600] train: 1.7435 test: 1.8411\n",
      "Error [0/700] train: 1.5471 test: 1.7321\n",
      "Error [0/800] train: 1.5829 test: 2.0068\n",
      "Error [0/900] train: 1.7403 test: 1.7047\n",
      "Error [0/1000] train: 1.7484 test: 1.6600\n",
      "Train accuracy 0.1094%\n",
      "Test accuracy 0.1133%\n",
      "Error [0/1100] train: 1.2879 test: 1.5842\n",
      "Error [0/1200] train: 1.6271 test: 1.9299\n",
      "Error [0/1300] train: 1.3786 test: 1.3602\n",
      "Error [0/1400] train: 1.6370 test: 1.8290\n",
      "Error [0/1500] train: 1.6509 test: 1.7395\n",
      "Train accuracy 0.0859%\n",
      "Test accuracy 0.0918%\n"
     ]
    }
   ],
   "source": [
    "# session configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "\n",
    "# work on GPU if available\n",
    "with tf.device(\"/gpu:0\"):\n",
    "\n",
    "    # start a new session\n",
    "    with tf.Session(graph=graphCNN, config=config) as session:  \n",
    "    \n",
    "        # initialize weights and bias variables\n",
    "        session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
    "        \n",
    "        # which nodes to fetch from the computation graph\n",
    "        fetch_train_nodes = {\n",
    "            'cost' : cost,\n",
    "            'optimizer' : optimizer \n",
    "        }\n",
    "        \n",
    "        for epoch in range(1):\n",
    "            \n",
    "            # shuffle data\n",
    "            permutation = np.random.permutation(X_train.shape[0])\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            \n",
    "            for i in range(num_iters):\n",
    "                X_batch = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "                y_batch = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                output_batch = session.run(fetch_train_nodes, feed_dict={x_input: X_batch, y_input: y_batch})\n",
    "                if(i % 100) == 0:\n",
    "                    train_costs.append(output_batch[\"cost\"])\n",
    "                    i_test = np.random.randint(X_test.shape[0], size=batch_size)\n",
    "                    X_batch_test = X_test[i_test]\n",
    "                    y_batch_test = y_test[i_test]\n",
    "                    output_batch_test = session.run(fetch_train_nodes, feed_dict={x_input: X_batch_test, y_input: y_batch_test})\n",
    "                    test_costs.append(output_batch_test[\"cost\"])\n",
    "                    \n",
    "                    print(\"Error [{}/{}] train: {:.4f} test: {:.4f}\".format(epoch, i, output_batch[\"cost\"], output_batch_test[\"cost\"]))\n",
    "                if(i % 500) == 0:\n",
    "                    print(\"Train accuracy {:.4f}%\".format(session.run(acc, feed_dict={x_input: X_batch, y_input: y_batch})))\n",
    "                    print(\"Test accuracy {:.4f}%\".format(session.run(acc, feed_dict={x_input: X_batch_test, y_input: y_batch_test})))\n",
    "                    \n",
    "\n",
    "        # check against test set\n",
    "        print(\"Test accuracy \", session.run(acc, feed_dict={x_input: X_train, y_input: y_train}))\n",
    "        print(\"Test accuracy \", session.run(acc, feed_dict={x_input: X_test, y_input: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h2>Abgabe</h2>\n",
    "\n",
    "<p>Das von Ihnen erstellte Notebook und die PDF Datei m&uuml;ssen sp&auml;testens bis zum 21. Januar 2018 um 23:59 UTC+1 ;) per E-Mail an&nbsp;<a href=\"mailto:hezel@htw-berlin.de\" target=\"_blank\">hezel@htw-berlin.de</a>&nbsp;eingesendet werden. Verwenden Sie als Betreff bitte &quot;CV1718 &Uuml;bung4 &lt;NAME&gt;&quot; und als Notebook Name &quot;CV1718_Ue4_Tensorflow_ConvNet_CIFAR_NAME.ipynb&quot; sowie &quot;CV1718_Ue4_Tensorflow_ConvNet_CIFAR_NAME.pdf&quot; f&uuml;r die PDF. Bevor Sie mir eine Mail schicken, entfernen Sie bitte &uuml;ber &quot;Kernel&quot; -&gt; &quot;Restart and Clear Output&quot; s&auml;mtlichen von Python erstellten Inhalt und speichern anschlie&szlig;end das Notebook &quot;File&quot; -&gt; &quot;Save and Checkpoint&quot;.</p>\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
